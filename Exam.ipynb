{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "\n",
    "\n",
    "- This exam is open book/open browser. That means you are allowed to use any resources to answer the questions, except communication with any other person.\n",
    "\n",
    "\n",
    "- Once finished you should upload your exam to your personal submission folder in the TUBS-Cloud, that we have assigned to you. If you haven't been assigned to one, tell us now!\n",
    "\n",
    "\n",
    "- The first part consists of 10 multiple choice questions, each increasing your final score by 5 points when answered corretly.\n",
    "\n",
    "\n",
    "- More details on the theory part and programming part are given below. \n",
    "\n",
    "\n",
    "- Please read the instructions carefully.\n",
    "\n",
    "\n",
    "- Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Machine Learning theory (50/100 pts.)\n",
    "Please answer the multiple choice questions below. For each question, there is exactly ***ONE*** correct answer. Selecting multiple answers or the wrong answer results in 0 points for that question. Each question will have 4 different answer choices, numbered from 1-4. After each question, there will be a cell that you can use to place your answer. You just need to enter ***the number of the answer*** to that question, e.g. a number between 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (5pts.)\n",
    "\n",
    "A ***regression model***(e.g. linear regression) expects an input of type $\\mathbf x \\ \\epsilon \\ \\mathbb R^{d}$, where $\\mathbf x$ can have multiple dimensions/features(e.g. d=4 features/columns).\n",
    "What dimensionality and range of possible values does a label in a regression dataset have ?\n",
    "\n",
    "1. $y \\ \\epsilon \\ \\{0, 1\\}$, so y is either a scalar number equal to \"1\", or equal to \"0\" which represents one of two classes.\n",
    "\n",
    "\n",
    "2. $y \\ \\epsilon \\ \\mathbb R$, so y can represent any scalar number on the real line.\n",
    "\n",
    "\n",
    "3. $y \\ == \\ \\mathbf x$, so y has the same dimensionality and value as $\\mathbf x$.\n",
    "\n",
    "\n",
    "4. $y \\ \\epsilon \\ \\emptyset$, so there is no y-labels in a regresion task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5pts.)\n",
    "\n",
    "A ***binary classification model***(e.g. binary nearest centroid classifier) expects an input of type $\\mathbf x \\ \\epsilon \\ \\mathbb R^{d}$, where $\\mathbf x$ can have multiple dimensions/features(e.g. d=4 features/columns).\n",
    "Select the type of a label \"$y$\" in a typical classification task:\n",
    "\n",
    "1. $y \\ \\epsilon \\ \\{0, 1\\}$, so y is either a scalar number equal to \"1\", or equal to \"0\" which represents one of two classes.\n",
    "\n",
    "\n",
    "2. $y \\ \\epsilon \\ \\mathbb R$, so y can represent any scalar number on the real line.\n",
    "\n",
    "\n",
    "3. $y \\ == \\ \\mathbf x$, so y has the same dimensionality and value as $\\mathbf x$.\n",
    "\n",
    "\n",
    "4. $y \\ \\epsilon \\ \\emptyset$, so there is no y-labels in a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (5pts.)\n",
    "\n",
    "A ***clustering model***(e.g. k-neigherst neighbor clustering) expects an input of type $\\mathbf x \\ \\epsilon \\mathbb \\ R^{d}$, where $\\mathbf x$ can have multiple dimensions/features(e.g. d=4 features/columns).\n",
    "Select the type of a label \"$y$\" in a typical clustering task:\n",
    "\n",
    "1. $y \\ \\epsilon \\ \\{0, 1\\}$, so y is either a scalar number equal to \"1\", or equal to \"0\", which represents one of two classes.\n",
    "\n",
    "\n",
    "2. $y \\ \\epsilon \\ \\mathbb R$, so y can represent any scalar number on the real line.\n",
    "\n",
    "\n",
    "3. $y \\ \\epsilon \\ \\mathbf x_{input}$, so y has the same dimensionality and value as $\\mathbf x$.\n",
    "\n",
    "\n",
    "4. $y \\ \\epsilon \\ \\emptyset$, so there is no y-label in a clustering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5pts.)\n",
    "\n",
    "Which of these statements about the loss function is true ? \n",
    "\n",
    "1. The loss function is a way of measuring the performance of our machine learning model, which is used to optimize it.\n",
    "\n",
    "\n",
    "2. The loss function and the error function are 2 separate things.\n",
    "\n",
    "\n",
    "3. There exists 1 loss function that is used by every single machine learning algorithm.\n",
    "\n",
    "\n",
    "4. The loss function is independent of our training data, which means that we don't need our training data to compute the loss of our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (5pts.)\n",
    "\n",
    "Which of these statements about the learning rate $\\alpha$ is true ? \n",
    "\n",
    "1. A large learning rate might result in never reaching the minimum of a loss function because our steps towards the minimum get too small to actually reach it. \n",
    "\n",
    "\n",
    "2. A small learning rate might result in \"overshooting\" the minimum of a loss function because we might \"overshoot\" the minimum.\n",
    "\n",
    "\n",
    "3. The learning rate is the step size that our gradient descent algorithm takes towards the direction of greatest decrease of our loss function, with respect to our model parameters.\n",
    "\n",
    "\n",
    "4. There learning rate has no impact on the optimization process of our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (5pts.)\n",
    "\n",
    "A confusion matrix let's us compute a variety of metrics that can be used to measure the performance of our trained classification model. \n",
    "It usually consists of 4 fields, namely:\n",
    "\n",
    "- True positives  / TP: The number of points that our model has predicted to be from class \"1\"(sometimes referred to as \"positive\" class), while their actual label was class \"1\".\n",
    "\n",
    "\n",
    "- False positives / FP: The number of points that our model has predicted to be from class \"1\", while their actual label was class \"0\"(sometimes referred to as \"negative class\").\n",
    "\n",
    "\n",
    "- False negatives / FN: The number of points that our model has predicted to be from class \"0\", while their actual label was class \"1\".\n",
    "\n",
    "\n",
    "- True negatives  / TN: The number of points that our model has predicted to be from class \"0\", while their actual label was class \"0\".\n",
    "\n",
    "How is the accuracy computed, when given the elements of our confusion matrix ? \n",
    "\n",
    "### NOTE: \"Total\" refers to the total amount of points of the whole dataset(training set or test set).\n",
    "\n",
    "1. $Accuracy = \\frac{TP+TN}{Total}$\n",
    "\n",
    "\n",
    "2. $Accuracy = \\frac{TP}{TP+FP}$\n",
    "\n",
    "\n",
    "3. $Accuracy = \\frac{TP}{TP+FN}$\n",
    "\n",
    "\n",
    "4. $Accuracy = \\frac{TP + FP + FN + TN}{Total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (5pts.)\n",
    "\n",
    "Which of the following classification algorithms uses the distance between a new point x and the class means of multiple classes, as a decision rule for classification ?\n",
    "\n",
    "1. Binary Logisitc Regression classifier\n",
    "\n",
    "\n",
    "2. Binary K-nearest centroid classifier\n",
    "\n",
    "\n",
    "3. Convolutional Neural Network\n",
    "\n",
    "\n",
    "4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (5pts.)\n",
    "\n",
    "When preparing the dataset for our learning algorithm, which of these steps does ***not*** help to achieve a better training process, in terms of efficiency, robustness, performance etc.\n",
    "\n",
    "\n",
    "1. Standardizing our features/columns.\n",
    "\n",
    "\n",
    "2. Dropping features that don't correlate with our label while keeping the ones that do correlate with the labels.\n",
    "\n",
    "\n",
    "3. Dropping all data points that belong to a certain label together with their labels. So at the end our dataset consists of only points from  a single class that we use for training.\n",
    "\n",
    "\n",
    "4. Training a machine learning model on different sets of hyperparameters(e.g. learning rates) and chossing the set that increases the test performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 (5pts.)\n",
    "\n",
    "When training a machine learning model, we usually do a training and test set split. What do we use the training and test set for ?\n",
    "\n",
    "1. We use both the training set and the test set to train our model, and evaluate it's generalization performance on the accuracy of the concatenation of both data sets.\n",
    "\n",
    "\n",
    "2. We use the training set to train our model and the test set to estimate it's generalization performance. That is, it's prediction accuracy on unseen data points.\n",
    "\n",
    "\n",
    "3. We use the training and test split only to safe up computational time.\n",
    "\n",
    "\n",
    "4. We use the test set to train our model and the training set to estimate it's generalization performance. That is, it's prediction accuracy on unseen data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 (5pts.)\n",
    "\n",
    "Which of these statements about convex funtions is true ? \n",
    "\n",
    "1. Optimizing a convex function with respect to a certain variable(e.g. some weight variable w), will result in a ***local*** optimum.\n",
    "\n",
    "\n",
    "2. Convex functions are not differentiable and thus not optimizable.\n",
    "\n",
    "\n",
    "3. Optimizing a convex function with respect to a certain variable(e.g. some weight variable w), will result in a ***global*** optimum.\n",
    "\n",
    "\n",
    "4. There is no single loss/error function used in machine learning model that is a convex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Python programming exercises (50/100 pts.)\n",
    "For the programming exercises, please follow the following:\n",
    "- follow the instructions for each exercise\n",
    "- use the provided blocks to write your code, and do not delete any other code lines. \n",
    "- you can add cells for testing, but we will only evaluate what is in the block!\n",
    "- it is not necessary, but you may add comments to your code, explaining what your were thinking and trying to do. If the solution is not correct, we still can give you points based on your ideas!\n",
    "- do not hesistate to ask if something is not clear or you need help :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Pandas(10 pts.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to import the modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Parse the dataset population.csv into a pandas DataFrame and clean (5 pts.)\n",
    "\n",
    "Complete the function `getData(path)`, which reads the file specified in the `path` input. The function should return a pandas `DataFrame`, containing the data in _population.csv_. The dataset holds data from the population of Barcelona over multiple years. For the following questions, we want to only use data from the year 2017. Therefore clean the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(path):\n",
    "    \n",
    "    data = pd.read_csv(path ,header = 0)\n",
    "    data = data[data.iloc[:,0] == 2017]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getData('population.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>District.Code</th>\n",
       "      <th>District.Name</th>\n",
       "      <th>Neighborhood.Code</th>\n",
       "      <th>Neighborhood.Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>1</td>\n",
       "      <td>el Raval</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-4</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>2</td>\n",
       "      <td>el Barri Gòtic</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-4</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>3</td>\n",
       "      <td>la Barceloneta</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-4</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciutat Vella</td>\n",
       "      <td>4</td>\n",
       "      <td>Sant Pere, Santa Caterina i la Ribera</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-4</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>Eixample</td>\n",
       "      <td>5</td>\n",
       "      <td>el Fort Pienc</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-4</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  District.Code District.Name  Neighborhood.Code  \\\n",
       "0  2017              1  Ciutat Vella                  1   \n",
       "1  2017              1  Ciutat Vella                  2   \n",
       "2  2017              1  Ciutat Vella                  3   \n",
       "3  2017              1  Ciutat Vella                  4   \n",
       "4  2017              2      Eixample                  5   \n",
       "\n",
       "                       Neighborhood.Name Gender  Age  Population  \n",
       "0                               el Raval   Male  0-4         224  \n",
       "1                         el Barri Gòtic   Male  0-4          50  \n",
       "2                         la Barceloneta   Male  0-4          43  \n",
       "3  Sant Pere, Santa Caterina i la Ribera   Male  0-4          95  \n",
       "4                          el Fort Pienc   Male  0-4         124  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Parse the dataset population.csv into a pandas DataFrame and clean (5 pts.)\n",
    "\n",
    "Complete the function `getData(path)`, which reads the file specified in the `path` input. The function should return a pandas `DataFrame`, containing the data in _population.csv_. The dataset holds data from the population of Barcelona over multiple years. For the following questions, we want to only use data from the year 2017. Therefore clean the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalPopulation(data):\n",
    "    \n",
    "    population = ### Your code here ###\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTotalPopulation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.0 Preprocessing (10 pts.)\n",
    "### a) - Scaling (5 pts.)\n",
    "For this exercise, you are given three datasets ```X1,X2,X3```. \n",
    "\n",
    "The goal of this exercise is to \n",
    " - normalize each of the ```X1,X2,X3```, by implementing the ```normalize``` function below and using it with an appropriate ```X_property```.\n",
    "\n",
    "The ```X_property``` determines, whether the data is sparse, or has outliers or none of the two. From this you must derive which of the imported Scalers to use. If ```X_property``` is ```None```, use the ```sklearn.preprocessing.StandardScaler```.\n",
    "\n",
    "To complete the ```normalize``` function, you should\n",
    " - create the right scaler object for each possible ```X_property```\n",
    " - create a pipeline object to which you add the scaler\n",
    " - fit the pipeline to your input data ```X```\n",
    " - return the transformed ```X```\n",
    "\n",
    "\n",
    "**After having implemented the function, please CORRECT AND UNCOMMENT the given print statements to see if your function runs withouot errors on the given data arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26726124 -1.22474487  0.53916387]\n",
      " [ 1.33630621  1.22474487  0.86266219]\n",
      " [-1.06904497  0.         -1.40182605]]\n",
      "[[ 1.  0.  2.]\n",
      " [-1.  2.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.         -0.33333333  0.8       ]\n",
      " [ 0.02        1.          1.        ]\n",
      " [ 0.005       0.33333333 -0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "X_properties = [None, 'sparse', 'has_outliers']\n",
    "\n",
    "# data is not sparse and has not got any outliers\n",
    "X1 = np.array([[ 1., -1.,  2.],\n",
    "     [ 2.,  3.,  2.5],\n",
    "    [ 0.5,  1., -1.]])\n",
    "\n",
    "# data is sparse\n",
    "X2 = np.array([[ 1., 0.,  2.],\n",
    "     [ 0.,  3.,  0.],\n",
    "    [ 0.5,  0., 0.]])\n",
    "\n",
    "# data has outliers\n",
    "X3 = np.array([[ 100., -1.,  2.],\n",
    "     [ 2.,  3.,  2.5],\n",
    "    [ 0.5,  1., -1.]])\n",
    "\n",
    "def normalize(X, X_property=None):\n",
    "    if X_property not in [None, 'sparse', 'has_outliers']:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if X_property == None: \n",
    "        pipeLine = make_pipeline(StandardScaler()) #create pipeline with argument standard scaler \n",
    "        pipeLine.fit(X)#fit the standar scaler pipeline on the given X\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    elif X_property == 'sparse':\n",
    "        pipeLine = make_pipeline(RobustScaler())# in this case we use RobustScaler because of the data distribution\n",
    "        pipeLine.fit(X)\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    elif X_property == 'has_outliers':\n",
    "        pipeLine = make_pipeline(MaxAbsScaler())# in this case we use MaxAbsScaler because the dataset includes outliers\n",
    "        pipeLine.fit(X)\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    pass\n",
    "    #<<<YOUE CODE ENDS <<<\n",
    "\n",
    "print(normalize(X1, None))\n",
    "# todo replace 'REPLACE_THIS_WITH_CORRECT_VALUE' with correct value\n",
    "print(normalize(X2, 'sparse'))\n",
    "print(normalize(X3, 'has_outliers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Imputation (5 pts.)\n",
    "For this exercise, you are given the dataset ```X_nan```.\n",
    "\n",
    "\n",
    "You should now extend your ```normalize``` function, such that it can replace ```np.nan``` values by the mean value of the corresponding column. For that the function should now **additionally**\n",
    " - create a simple imputer with the right arguments\n",
    " - add the imputer object to the pipeline\n",
    " - fit the pipeline to your input data ```X```\n",
    " - return the transformed ```X```\n",
    " \n",
    " \n",
    "Note: You can copy the normalize function from above.\n",
    "\n",
    "\n",
    "**After having implemented the function, please uncomment the last print statement in this cell, to see if your function runs withouot errors on the given data array.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487 -1.22474487  0.        ]\n",
      " [-1.22474487  1.22474487  0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "X_properties = [None, 'sparse', 'has_outliers']\n",
    "\n",
    "# data has missing values\n",
    "Xnan = np.array([[ 100., -1.,  np.nan],\n",
    "     [ 2.,  3.,  np.nan],\n",
    "    [ np.nan,  1., -1.]])\n",
    "\n",
    "def normalize(X, X_property=None):\n",
    "    if X_property not in X_properties:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #>>> YOUR CODE HERE >>>\n",
    "    \n",
    "    \n",
    "    if X_property == None: \n",
    "        pipeLine = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'),StandardScaler()) #create pipeline with argument standard scaler \n",
    "        pipeLine.fit(X)#fit the standar scaler pipeline on the given X\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    elif X_property == 'sparse':\n",
    "        pipeLine = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'),RobustScaler())# in this case we use RobustScaler because of the data distribution\n",
    "        pipeLine.fit(X)\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    elif X_property == 'has_outliers':\n",
    "        pipeLine = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'),MaxAbsScaler())# in this case we use MaxAbsScaler because the dataset includes outliers\n",
    "        pipeLine.fit(X)\n",
    "        return pipeLine.transform(X) #transforms the dataset\n",
    "    pass\n",
    "    #<<<YOUE CODE ENDS <<<\n",
    "\n",
    "print(normalize(Xnan, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Feature Selection (10 pts.)\n",
    "### a) Removing redundant features (5 pts.)\n",
    "\n",
    "You are given a labelled dataset ```X, y``` for a binary classification problem. \n",
    "It consists of 4 features, where 2 are redundant. The goal of this exercise is to remove the two redundant features using ```sklearn.feature_selection.SelectFromModel``` and ```sklearn.svm.LinearSVC``` with the 'l1'-penalty, as shown in the lecture. For this you should implement the function ```remove_redundant_features(X)``` as follows:\n",
    " - create a linear support vector classifier using the right values for 'penalty' and 'dual'\n",
    " - normalize the data using your function from above (if you implemented it, otherwise do not normalize the data) \n",
    " - fit the SelectFromModel transformer\n",
    " - return a boolean mask where Features are masked out that should be removed\n",
    "\n",
    "\n",
    "**After having implemented the function, please uncomment the last print statement in this cell, to see if your function runs withouot errors on the given data array.**\n",
    "\n",
    "\n",
    "Hints: \n",
    "- remember to set the right values for penalty and dual in the LinearSVC\n",
    "- using threshold='median' will make sure that SelectFromModel reduces the number of features to two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.05383855, -1.02754411, -0.32929388,  0.82600732],\n",
       "       [ 1.56931739,  1.306542  , -0.23938468, -0.3313761 ],\n",
       "       [-0.35885569, -0.69102126, -1.22532865,  1.65214494],\n",
       "       [-0.1368559 ,  0.46093758,  1.89691056, -2.2813861 ],\n",
       "       [-0.04862909,  0.50230075,  1.77872961, -2.17105282],\n",
       "       [ 2.30450019,  1.33970234, -2.24075406,  1.83991037],\n",
       "       [-0.40925706,  0.32423734,  2.23245441, -2.58590856],\n",
       "       [-0.7173148 , -0.88602706, -0.83311649,  1.31217492],\n",
       "       [ 1.06173727,  1.05688647,  0.40238353, -0.91916686],\n",
       "       [ 1.48052803,  1.28131775, -0.06692233, -0.50833095],\n",
       "       [ 0.53479393,  0.21164583, -0.84389686,  0.82584805],\n",
       "       [ 1.38991764,  1.22541983,  0.01066604, -0.56772453],\n",
       "       [-1.79532002, -1.46361184,  0.37531604,  0.25415746],\n",
       "       [-1.26898369, -0.68933585,  1.39175475, -1.20757158],\n",
       "       [-1.56826626, -0.87306613,  1.65095783, -1.40735658],\n",
       "       [-0.42795824, -0.72216846, -1.12868571,  1.56070438],\n",
       "       [ 0.73644435,  0.28247903, -1.19137182,  1.17329352],\n",
       "       [-1.42922002, -1.29267194, -0.1173572 ,  0.71479373],\n",
       "       [ 0.49429823,  0.36783185, -0.21800395,  0.07123641],\n",
       "       [-1.32933233, -1.19300559, -0.07873421,  0.6273745 ],\n",
       "       [ 2.25661188,  1.17077461, -2.65461268,  2.36867367],\n",
       "       [-2.25553963, -1.69200458,  0.95057302, -0.27062383],\n",
       "       [-1.92487377, -1.11327862,  1.89033108, -1.5598485 ],\n",
       "       [-1.20592943, -0.64760425,  1.34700796, -1.17762637],\n",
       "       [-0.6795874 , -0.32683618,  0.8834691 , -0.81680628],\n",
       "       [ 0.13979096,  0.04668349, -0.24878048,  0.25058844],\n",
       "       [ 1.15509316,  0.56372286, -1.47487037,  1.35536951],\n",
       "       [-2.52343407, -1.45612516,  2.48904785, -2.05832072],\n",
       "       [-0.17593681,  0.44205183,  1.94742094, -2.3279946 ],\n",
       "       [-1.5332749 , -0.84467713,  1.6431949 , -1.4117586 ],\n",
       "       [-0.38919817, -0.67826129, -1.09662143,  1.50575249],\n",
       "       [-1.00544254, -1.05463759, -0.55658056,  1.08659413],\n",
       "       [-1.14658127, -1.10644979, -0.32066954,  0.85239186],\n",
       "       [ 0.08896214,  0.08714206,  0.0291022 , -0.07133524],\n",
       "       [-0.43131517, -0.74920203, -1.20727346,  1.65882246],\n",
       "       [ 2.38869353,  1.7240021 , -1.22823431,  0.55942643],\n",
       "       [ 0.88629356,  0.52576893, -0.82741596,  0.66530077],\n",
       "       [-1.57127256, -0.81296687,  1.85571007, -1.65830375],\n",
       "       [ 0.24472415, -0.45390127, -2.18347304,  2.59123946],\n",
       "       [ 0.55448398,  0.17593   , -1.01695056,  1.03110238],\n",
       "       [ 1.07954187,  0.50047696, -1.46447269,  1.3727107 ],\n",
       "       [ 1.19664076,  1.13446163,  0.26843823, -0.80804463],\n",
       "       [ 0.23085877,  0.64701888,  1.44901594, -1.87653774],\n",
       "       [-0.86023823, -0.39769634,  1.17059844, -1.09831681],\n",
       "       [ 0.01722979,  0.1094067 ,  0.30759353, -0.38566776],\n",
       "       [-1.08324727, -1.09939128, -0.47936995,  1.02255619],\n",
       "       [-0.95548162, -1.03260487, -0.62804104,  1.15466088],\n",
       "       [ 1.85605469,  1.46098002, -0.55818081, -0.05319823],\n",
       "       [-1.50077611, -1.2827436 ,  0.12037048,  0.4505902 ],\n",
       "       [-1.14054824, -0.63922566,  1.18674111, -1.00634985],\n",
       "       [-0.98021491, -1.03398089, -0.56156012,  1.08266027],\n",
       "       [-1.72067112, -0.99325144,  1.69607038, -1.40210053],\n",
       "       [ 0.86561977,  0.96335953,  0.65992405, -1.15806823],\n",
       "       [-1.73597973, -1.37111696,  0.50688537,  0.06845616],\n",
       "       [-0.11708689, -0.05396947,  0.15985512, -0.15013844],\n",
       "       [-0.53355799, -0.79864786, -1.07525068,  1.53703587],\n",
       "       [-1.20946429, -1.13714057, -0.24038388,  0.77861318],\n",
       "       [ 1.10009583,  1.09267506,  0.40910605, -0.94275087],\n",
       "       [ 0.36016958,  0.71768635,  1.30857636, -1.75518644],\n",
       "       [ 1.94087643,  0.88439534, -2.68317954,  2.52983424],\n",
       "       [ 0.89075877,  0.97650004,  0.63067073, -1.13207427],\n",
       "       [ 1.02703224,  0.63174629, -0.88541844,  0.68057323],\n",
       "       [ 1.25896077,  0.6635226 , -1.44723231,  1.2798899 ],\n",
       "       [ 0.68811892,  0.30446853, -0.9809438 ,  0.93343952],\n",
       "       [ 0.4176729 ,  0.73594772,  1.2031659 , -1.64832073],\n",
       "       [-1.43534875, -0.73426524,  1.72251306, -1.54851014],\n",
       "       [ 0.06194498,  0.55253544,  1.6253749 , -2.02632079],\n",
       "       [-0.72427983, -0.43066755,  0.67287309, -0.53963044],\n",
       "       [ 0.73019848,  0.88948365,  0.80742726, -1.28568005],\n",
       "       [-0.71352532, -0.84321213, -0.70426956,  1.15199146],\n",
       "       [-1.64060704, -0.94116325,  1.63631177, -1.36045573],\n",
       "       [ 1.28938375,  0.68720469, -1.45724683,  1.28008347],\n",
       "       [ 1.56010259,  1.30373602, -0.22210005, -0.34898484],\n",
       "       [ 0.54914434,  0.30877317, -0.56811307,  0.480502  ],\n",
       "       [ 0.55600276,  0.19375402, -0.96314239,  0.96423311],\n",
       "       [-1.61453273, -0.83806958,  1.89792447, -1.69302842],\n",
       "       [ 0.94800532,  0.99630897,  0.53104737, -1.03223274],\n",
       "       [ 1.74147706,  0.97687358, -1.80921926,  1.53313849],\n",
       "       [ 1.09180466,  0.55831966, -1.31089865,  1.17869556],\n",
       "       [-1.42695838, -0.756006  ,  1.62748917, -1.43483867],\n",
       "       [-0.48949935, -0.74717988, -1.03371682,  1.46830827],\n",
       "       [ 1.4991983 ,  0.77813969, -1.76254901,  1.57233676],\n",
       "       [-1.36716377, -0.7635164 ,  1.43140126, -1.21722043],\n",
       "       [ 1.18739045,  1.12210055,  0.254643  , -0.78736523],\n",
       "       [-1.51042899, -0.79383636,  1.74355119, -1.54446032],\n",
       "       [-0.02185231, -0.51103632, -1.60499282,  1.98522348],\n",
       "       [ 1.99805321,  1.51990078, -0.77336118,  0.15513175],\n",
       "       [-2.39955005, -1.3731412 ,  2.40437346, -2.00347738],\n",
       "       [ 1.25291095,  0.70385192, -1.29826331,  1.09885263],\n",
       "       [ 1.35970566,  0.84609208, -1.14052601,  0.86199147],\n",
       "       [ 0.62453032,  0.4057089 , -0.46809422,  0.32725188],\n",
       "       [-0.92169432, -0.97763681, -0.54561186,  1.03967019],\n",
       "       [ 0.25995672,  0.65574086,  1.3939833 , -1.82037691],\n",
       "       [-0.9311259 , -0.52878508,  0.94621814, -0.79371387],\n",
       "       [ 1.20852705,  1.1374615 ,  0.24412056, -0.78284083],\n",
       "       [-2.24181979, -1.24868955,  2.35790248, -2.00918545],\n",
       "       [ 0.57304248,  0.36205368, -0.46281432,  0.34129395],\n",
       "       [-0.3751207 , -0.14951801,  0.58846525, -0.57500215],\n",
       "       [ 1.59488828,  0.78025563, -2.0302233 ,  1.86378908],\n",
       "       [-0.14994102, -0.56603681, -1.41693293,  1.80474148]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X, y = make_classification(n_samples=100, \n",
    "                           n_classes=2,\n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=2,\n",
    "                           random_state=42)\n",
    "\n",
    "def remove_redundant_features(X):\n",
    "    #>>> YOUR CODE HERE >>>\n",
    "    linearSVC = LinearSVC(penalty='l1', dual=False)\n",
    "    normalize(X)\n",
    "    selector = SelectFromModel(linearSVC, threshold='median')\n",
    "    selector.fit(X,y)\n",
    "    return X\n",
    "    pass\n",
    "    #<<< YOUE CODE ENDS <<<\n",
    "\n",
    "mask = remove_redundant_features(X)\n",
    "mask\n",
    "\n",
    "#couldnt really understand the mask statement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Feature Selection (10 pts.)\n",
    "### b) Refitting using reduced set of features (5 pts.)\n",
    "You are given the same labelled dataset ```X, y``` as in the previous exercise ```3a)```.\n",
    "\n",
    "After having implemented ```remove_redundant_featuers``` function above, you should now\n",
    "\n",
    " - apply ```remove_redundant_featuers``` to ```X``` **and transform  ```X``` using the result** (Hint: check the shape of ```X```, and consider using the transpose .T)\n",
    " - create train and test split using a random seed\n",
    " - normalize the data using your function from above (if you implemented it, otherwise do not normalize the data) \n",
    " - now fit a LinearSVC on the reduced and normalized dataset ```Xtrain``` and \n",
    " - print the result of calling its score method on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, \n",
    "                           n_classes=2,\n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=2,\n",
    "                           random_state=42)\n",
    "\n",
    "#>>> YOUR CODE HERE >>>\n",
    "#remove_redundant_features(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42) #\n",
    "normalize(X_train), normalize(X_test)\n",
    "linearSVC_2 = linearSVC = LinearSVC()\n",
    "linearSVC_2.fit(X_train, Y_train)\n",
    "print(linearSVC_2.score(X_test, Y_test))\n",
    "\n",
    "\n",
    "#     pass\n",
    "#<<< YOUE CODE ENDS <<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Hyperparameter Optimization (10 pt)\n",
    "\n",
    "You are given the same labelled dataset ```X, y``` as before and should now train a ```sklearn.svm.SVC estimator```. For that, a parameter grid is given, from which you should pick the parameter combination, which performs best on the classification task, given a handicap we provide. The handicap is the restriction of the dataset to only 20 percent of the data, which is already implemented for you via the train_test_split.\n",
    "\n",
    "Your task is to implement the ```report_best_params_w_score``` function by \n",
    " - normalizing ```Xtrain``` using your function from task ```3.0 a)``` or an appropriate sklearn function\n",
    " - doing a GridSearch using the import GridSearchCV object over the parameter grid (i.e. by properly calling its ```.fit``` method\n",
    " - printing the best parameters obtained\n",
    " - returning the best_estimator (Hint: see GridSearchCV's corresponding attribute in the documentation)\n",
    " \n",
    "\n",
    "Finally, you should use the returned estimator, to evaluate the test score, by calling its .score method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def report_best_params_w_score():\n",
    "    X, y = make_classification(n_samples=100, \n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=2,\n",
    "                           random_state=42)\n",
    "\n",
    "    param_grid = {'C': [0.1,1.,10], 'kernel': ['rbf', 'linear']}\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, \n",
    "                                                    random_state=42, \n",
    "                                                    test_size=0.8)\n",
    "    #>>> YOUR CODE HERE >>>\n",
    "    normalize(Xtrain)\n",
    "    mySVC = SVC()\n",
    "    mySVC = GridSearchCV(mySVC,param_grid,cv=None, scoring=None,return_train_score=True)\n",
    "    mySVC.fit(X_train,Y_train)\n",
    "    print(mySVC.best_params_)\n",
    "    mySVC = mySVC.best_estimator_\n",
    "    return mySVC\n",
    "    pass\n",
    "    #<<< YOUE CODE ENDS <<<\n",
    "\n",
    "report_best_params_w_score()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Logistic Regression optimization: Applying Gradient Descent using Binary Cross Entropy Loss(10 pts.)\n",
    "\n",
    "- Implement the gradient descent algorithm on the Logistic Regression model as discussed in the lecture.\n",
    "- use 100 epochs(update steps)\n",
    "- use learning rate alpha of 0.0001\n",
    "- use the X dataset from below. Use the whole X and y arrays, no train test split needed here.\n",
    "- don't change the weight parameter(w) initialization and don't change the random seed\n",
    "- Use the \"log_loss\" function that gets imported below. It's the SkLearn implementation for the binary-cross entropy loss function.\n",
    "- Use the \"expit\" function that gets imported below. It's the SciPy implementation for the sigmoid function.\n",
    "- Plot the training loss over epochs\n",
    "\n",
    "#### The formula for the gradient of the binary-cross entropy loss function:\n",
    "- $\\nabla_{\\mathbf w^t} J(\\mathbf w^t) = \\frac{1}{N}\\mathbf X^T(\\sigma(\\mathbf X \\mathbf w^t)-\\mathbf y)$\n",
    "- Update rule Gradient Descent: $\\mathbf{w}^t=\\mathbf{w}^t-\\alpha \\nabla_{\\mathbf w} J(\\mathbf w^t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.metrics import log_loss \n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.rand(X.shape[1],1)\n",
    "\n",
    "loss = []\n",
    "\n",
    "for i in range(n_epochs):        \n",
    "    # Compute the gradient\n",
    "\n",
    "    ### Your code here ###\n",
    "            \n",
    "    # Apply the udpate step\n",
    "    ### Your code here ###\n",
    "            \n",
    "    # store the loss in the loss array (optional)\n",
    "    ### Your code here ###\n",
    "    \n",
    "# Plot the loss\n",
    "plt.plot(loss);\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
